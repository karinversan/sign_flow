services:
  triton:
    image: nvcr.io/nvidia/tritonserver:24.10-py3
    command: ["tritonserver", "--model-repository=/models"]
    ports:
      - "8001:8000" # HTTP inference
      - "8002:8001" # gRPC inference
      - "8003:8002" # Metrics
    volumes:
      - ./model_repository:/models:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
